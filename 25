def run_pipeline(root_window):
    """
    Ejecuta todo el pipeline de generación de reportes.
    root_window: necesario para filedialog desde una app CTk.
    """
    conn = sqlite3.connect(DB_PATH)

    try:
        # 1. Cargar movimientos y unir con catálogo para obtener CLAVE_BANXICO
        df_mov = pd.read_sql("""
            SELECT 
                m.CONTRAPARTE,
                m.EMISION,
                m.TITULOS,
                m.ISIN,
                m.ENVIO_RECEPCION,
                c.CLAVE_BANXICO
            FROM MOVIMIENTOS m
            LEFT JOIN CAT_CONTRAPARTES c ON m.CONTRAPARTE = c.CONTRAPARTE
        """, conn)

        if df_mov.empty:
            messagebox.showwarning("Advertencia", "No hay movimientos cargados.")
            return

        # Asegurar que TITULOS sea numérico
        df_mov['TITULOS'] = pd.to_numeric(df_mov['TITULOS'], errors='coerce')

        # 2. Agrupar por CLAVE_BANXICO + EMISION (no por CONTRAPARTE)
        df_agrupado = df_mov.groupby(['CLAVE_BANXICO', 'EMISION'], as_index=False).agg({
            'TITULOS': 'sum',
            'ISIN': 'first',
            'ENVIO_RECEPCION': 'first'
        })

        # 3. Eliminar posiciones neteadas a cero
        df_agrupado = df_agrupado[df_agrupado['TITULOS'] != 0].copy()



        if df_agrupado.empty:
            messagebox.showinfo("Información", "No hay posiciones abiertas (todo neteado a cero).")
            return

        # 4. Volver a unir con CAT_CONTRAPARTES para recuperar CONTRAPARTE
        # (por si hay más de una contraparte por CLAVE_BANXICO, tomamos la primera)
        df_final = df_agrupado.merge(
            df_mov[['CLAVE_BANXICO', 'CONTRAPARTE']].drop_duplicates(),
            on='CLAVE_BANXICO',
            how='left'
        )

        # 5. Clasificar como nacional o extranjero
        df_final['TIPO'] = df_final['EMISION'].apply(classify_emision)
        total_titulos = df_final['TITULOS'].sum()
        messagebox.showinfo("Total", f"Suma total de títulos (neto): {total_titulos:,}")


--------------------------------------------->>>
import pandas as pd
import sqlite3
from tkinter import filedialog, messagebox, simpledialog
from utils import normalize_text, classify_emision, extract_date_from_pip_filename

def run_pipeline(root_window):
    """
    Ejecuta todo el pipeline de generación de reportes.
    root_window: necesario para filedialog desde una app CTk.
    """
    conn = sqlite3.connect(DB_PATH)

    try:
        # 1. Cargar todos los movimientos y unir con catálogo para obtener CLAVE_BANXICO
        df_mov = pd.read_sql("""
            SELECT 
                m.CONTRAPARTE,
                m.EMISION,
                m.TITULOS,
                m.ISIN,
                m.ENVIO_RECEPCION,
                c.CLAVE_BANXICO,
                o.NOMBRE AS ORIGEN_NOMBRE
            FROM MOVIMIENTOS m
            LEFT JOIN CAT_CONTRAPARTES c ON m.CONTRAPARTE = c.CONTRAPARTE
            JOIN ORIGEN o ON m.ID_ORIGEN = o.ID_ORIGEN
        """, conn)

        if df_mov.empty:
            messagebox.showwarning("Advertencia", "No hay movimientos cargados.")
            return

        # Asegurar que TITULOS sea numérico
        df_mov['TITULOS'] = pd.to_numeric(df_mov['TITULOS'], errors='coerce')

        # 2. Agrupar por CLAVE_BANXICO + EMISION (neteo por entidad)
        df_agrupado = df_mov.groupby(['CLAVE_BANXICO', 'EMISION'], as_index=False).agg({
            'TITULOS': 'sum',
            'ISIN': 'first',
            'ENVIO_RECEPCION': 'first',
            'ORIGEN_NOMBRE': lambda x: ', '.join(x.dropna().unique())
        })

        # 3. Eliminar posiciones neteadas a cero
        df_agrupado = df_agrupado[df_agrupado['TITULOS'] != 0].copy()

        if df_agrupado.empty:
            messagebox.showinfo("Información", "No hay posiciones abiertas (todo neteado a cero).")
            return

        # 4. Volver a unir con CAT_CONTRAPARTES para recuperar CONTRAPARTE
        df_final = df_agrupado.merge(
            df_mov[['CLAVE_BANXICO', 'CONTRAPARTE']].drop_duplicates(),
            on='CLAVE_BANXICO',
            how='left'
        )

        # 5. Clasificar como nacional o extranjero
        df_final['TIPO'] = df_final['EMISION'].apply(classify_emision)
        total_titulos = df_final['TITULOS'].sum()
        messagebox.showinfo("Total", f"Suma total de títulos (neto): {total_titulos:,}")

        # 6. Enriquecer con catálogos
        cat_isin = pd.read_sql("SELECT PAPEL, CLAVE_ISIN FROM CAT_ISIN", conn)
        df_final = df_final.merge(cat_isin, left_on='EMISION', right_on='PAPEL', how='left')

        # 7. Cargar INFORME BONOS para AFORO
        bonos_path = filedialog.askopenfilename(
            parent=root_window,
            title="Selecciona el archivo 'INFORME BONOS'",
            filetypes=[("Archivos Excel", "*.xlsx *.xlsm *.xls")]
        )
        if not bonos_path:
            raise Exception("Archivo de INFORME BONOS no seleccionado.")

        df_bonos = pd.read_excel(bonos_path, sheet_name=0)
        df_bonos.rename(columns={'Name': 'EMISION', 'Haircut': 'AFORO'}, inplace=True)
        df_bonos['EMISION'] = df_bonos['EMISION'].apply(normalize_text)
        df_bonos['CLAVE_AFORO'] = "-" + df_bonos['Saldo inicial (tit.)'].astype(str) + " " + df_bonos['EMISION']

        df_final['CLAVE_AFORO'] = df_final['TITULOS'].astype(str) + " " + df_final['EMISION']
        df_final = df_final.merge(
            df_bonos[['CLAVE_AFORO', 'AFORO']],
            on='CLAVE_AFORO',
            how='left'
        )

        # 8. Completar AFORO faltante para EXTRANJEROS usando CSA
        missing_extr = df_final[
            (df_final['TIPO'] == 'EXTRANJERO') & (df_final['AFORO'].isna())
        ][['CONTRAPARTE', 'EMISION']]

        if not missing_extr.empty:
            df_csa = pd.read_sql("""
                SELECT CONTRAPARTE, EMISION, HAIRCUT, FECHA_LIQ
                FROM MOVIMIENTOS
                WHERE ID_ORIGEN = 1
            """, conn)
            df_csa['EMISION'] = df_csa['EMISION'].apply(normalize_text)
            df_csa = df_csa.sort_values('FECHA_LIQ', ascending=False)
            df_csa_dedup = df_csa.drop_duplicates(subset=['CONTRAPARTE', 'EMISION'])
            df_csa_dedup['HAIRCUT'] = df_csa_dedup['HAIRCUT'] * 100  # 0.900 → 9.0

            missing_extr = missing_extr.merge(
                df_csa_dedup[['CONTRAPARTE', 'EMISION', 'HAIRCUT']],
                on=['CONTRAPARTE', 'EMISION'],
                how='left'
            )

            for idx, row in missing_extr.iterrows():
                mask = (df_final['CONTRAPARTE'] == row['CONTRAPARTE']) & \
                       (df_final['EMISION'] == row['EMISION'])
                df_final.loc[mask, 'AFORO'] = row['HAIRCUT']

        # Guardar pendientes NACIONALES sin AFORO
        pendientes_nac = df_final[
            (df_final['TIPO'] == 'NACIONAL') & (df_final['AFORO'].isna())
        ][['CONTRAPARTE', 'EMISION', 'TITULOS']]
        if not pendientes_nac.empty:
            output_path = os.path.join(os.path.dirname(bonos_path), "pendientes_por_confirmar_colaterales_aforo.xlsx")
            pendientes_nac.to_excel(output_path, index=False)
            messagebox.showinfo("Pendientes", f"Archivo de pendientes nacional guardado:\n{output_path}")

        # 9. Cargar Vector PIP (con skiprows=[0,2])
        pip_path = filedialog.askopenfilename(
            parent=root_window,
            title="Selecciona el archivo Vector PIP",
            filetypes=[("Archivos Excel", "*.xlsx *.xlsm *.xls")]
        )
        if not pip_path:
            raise Exception("Archivo Vector PIP no seleccionado.")

        pip_date = extract_date_from_pip_filename(os.path.basename(pip_path))
        if not pip_date:
            raise Exception("No se pudo extraer la fecha del archivo PIP. Asegúrate del formato: PIPYYYYMMDDM.xls")

        sheet_name = f"{pip_date}_MD"
        df_pip = pd.read_excel(pip_path, sheet_name=sheet_name, skiprows=[0, 2])
        df_pip.rename(columns={'Emisora': 'EMISORA', 'Serie': 'SERIE', 'Tipo Valor': 'TIPO_VALOR', 'Precio Sucio': 'PRECIO_SUCIO'}, inplace=True)
        df_pip['EMISORA'] = df_pip['EMISORA'].apply(normalize_text)

        # Extraer solo las emisoras nacionales necesarias
        emisoras_nacionales = set(
            df_final[df_final['TIPO'] == 'NACIONAL']['EMISION']
            .str.split(' ', n=1, expand=True)[0]
            .apply(normalize_text)
            .unique()
        )

        df_pip_filtrado = df_pip[df_pip['EMISORA'].apply(normalize_text).isin(emisoras_nacionales)].copy()

        # Cargar catálogo de TIPO_VALOR
        df_tipos = pd.read_sql("SELECT EMISORA, TIPO_VALOR FROM CAT_TIPO_VALOR", conn)
        df_tipos['EMISORA'] = df_tipos['EMISORA'].apply(normalize_text)

        # Detectar emisoras nuevas en df_pip_filtrado
        emisoras_pip = set(df_pip_filtrado['EMISORA'].apply(normalize_text).unique())
        emisoras_cat = set(df_tipos['EMISORA'].apply(normalize_text).unique())
        nuevas_emisoras = emisoras_pip - emisoras_cat

        for emisora in nuevas_emisoras:
            if messagebox.askyesno("Emisora no encontrada",
                                   f"No se encontró la emisora '{emisora}' en el catálogo de Tipo Valor.\n¿Desea darla de alta?"):
                tipo_valor = simpledialog.askstring("Tipo Valor", f"Ingrese TIPO_VALOR para '{emisora}':")
                if tipo_valor is None:
                    messagebox.showwarning("Cancelado", f"No se dio de alta el tipo de valor para '{emisora}'.")
                    continue

                cursor = conn.cursor()
                cursor.execute("INSERT INTO CAT_TIPO_VALOR (EMISORA, TIPO_VALOR) VALUES (?, ?)",
                               (str(emisora), str(tipo_valor)))
                conn.commit()

        # Recargar catálogo actualizado
        df_tipos = pd.read_sql("SELECT EMISORA, TIPO_VALOR FROM CAT_TIPO_VALOR", conn)
        df_tipos['EMISORA'] = df_tipos['EMISORA'].apply(normalize_text)

        # Completar TIPO_VALOR en df_pip_filtrado
        df_pip_filtrado = df_pip_filtrado.merge(df_tipos[['EMISORA', 'TIPO_VALOR']], on='EMISORA', how='left')

        # Crear clave compuesta en df_pip_filtrado
        df_pip_filtrado['CLAVE_PIP'] = (
            df_pip_filtrado['EMISORA'] + " " +
            df_pip_filtrado['SERIE'] + " " +
            df_pip_filtrado['TIPO_VALOR']
        )

        # Extraer EMISORA y SERIE de df_final para nacionales
        emision_split = df_final['EMISION'].str.split(' ', n=1, expand=True)
        df_final['EMISORA_PARA_PIP'] = emision_split[0].apply(normalize_text)
        df_final['SERIE_PARA_PIP'] = emision_split[1]

        # Obtener TIPO_VALOR para df_final desde catálogo
        df_final = df_final.merge(
            df_tipos[['EMISORA', 'TIPO_VALOR']],
            left_on='EMISORA_PARA_PIP',
            right_on='EMISORA',
            how='left',
            suffixes=('', '_cat')
        )

        # Crear clave compuesta en df_final
        df_final['CLAVE_PARA_PIP'] = (
            df_final['EMISORA_PARA_PIP'] + " " +
            df_final['SERIE_PARA_PIP'] + " " +
            df_final['TIPO_VALOR']
        )

        # Cruzar con Vector PIP (solo nacionales)
        df_final = df_final.merge(
            df_pip_filtrado[['CLAVE_PIP', 'PRECIO_SUCIO']],
            left_on='CLAVE_PARA_PIP',
            right_on='CLAVE_PIP',
            how='left',
            suffixes=('', '_pip')
        )

        # 10. Cargar VMDS para precios faltantes (extranjeros)
        vmds_path = filedialog.askopenfilename(
            parent=root_window,
            title="Selecciona el archivo VMDS (CSV)",
            filetypes=[("CSV files", "*.csv")]
        )
        if not vmds_path:
            raise Exception("Archivo VMDS no seleccionado.")

        df_vmds = pd.read_csv(vmds_path, header=None, dtype=str)
        if df_vmds.shape[1] < 5:
            raise Exception("El archivo VMDS debe tener al menos 5 columnas.")

        df_vmds.rename(columns={0: 'EMISION_VMDS', 3: 'EMISORA_VMDS', 4: 'SERIE_VMDS'}, inplace=True)
        df_vmds['EMISORA_VMDS'] = df_vmds['EMISORA_VMDS'].apply(normalize_text)

        # Filtrar extranjeros sin precio
        mask_sin_precio = df_final['PRECIO_SUCIO'].isna()
        df_sin_precio = df_final[mask_sin_precio].copy()
        df_extranjeros_sin_precio = df_sin_precio[df_sin_precio['TIPO'] == 'EXTRANJERO'].copy()

        if not df_extranjeros_sin_precio.empty:
            df_extranjeros_sin_precio = df_extranjeros_sin_precio.merge(
                df_vmds[['EMISION_VMDS', 'EMISORA_VMDS', 'SERIE_VMDS']],
                left_on='EMISION',
                right_on='EMISION_VMDS',
                how='inner'
            )

            df_extranjeros_sin_precio = df_extranjeros_sin_precio.merge(
                df_tipos[['EMISORA', 'TIPO_VALOR']],
                left_on='EMISORA_VMDS',
                right_on='EMISORA',
                how='left',
                suffixes=('', '_cat_vmds')
            )

            df_extranjeros_sin_precio['CLAVE_VMDS_PIP'] = (
                df_extranjeros_sin_precio['EMISORA_VMDS'] + " " +
                df_extranjeros_sin_precio['SERIE_VMDS'] + " " +
                df_extranjeros_sin_precio['TIPO_VALOR']
            )

            df_extranjeros_sin_precio = df_extranjeros_sin_precio.merge(
                df_pip_filtrado[['CLAVE_PIP', 'PRECIO_SUCIO']],
                left_on='CLAVE_VMDS_PIP',
                right_on='CLAVE_PIP',
                how='left',
                suffixes=('', '_pip_vmds')
            )

            # Actualizar precios en df_final
            for idx, row in df_extranjeros_sin_precio.iterrows():
                mask = (df_final['EMISION'] == row['EMISION']) & (df_final['TIPO'] == 'EXTRANJERO')
                df_final.loc[mask, 'PRECIO_SUCIO'] = row['PRECIO_SUCIO_pip_vmds']

        # 11. Calcular campos finales
        df_final['PRECIO'] = df_final['PRECIO_SUCIO']
        df_final['PRECIO_TOTAL'] = df_final['PRECIO'] * df_final['TITULOS']
        df_final['VAL_MERCA'] = (df_final['PRECIO_TOTAL'] / 1000).round(0).abs()
        df_final['RECIBE_ENTREGA'] = df_final['TITULOS'].apply(lambda x: 'E' if x < 0 else 'R')

        # 12. Separar nacional y extranjero
        nacional = df_final[df_final['TIPO'] == 'NACIONAL'].copy()
        extranjero = df_final[df_final['TIPO'] == 'EXTRANJERO'].copy()

        # Seleccionar columnas requeridas
        cols = [
            'CLAVE_BANXICO', 'CONTRAPARTE', 'EMISION', 'TITULOS',
            'RECIBE_ENTREGA', 'CLAVE_ISIN', 'AFORO', 'PRECIO',
            'PRECIO_TOTAL', 'VAL_MERCA', 'ORIGEN_NOMBRE'
        ]
        nacional = nacional[cols]
        extranjero = extranjero[cols]

        # 13. Guardar en DB (con TRUNCATE)
        nacional.to_sql('REPORTE_NACIONAL', conn, if_exists='replace', index=False)
        extranjero.to_sql('REPORTE_EXTRANJERO', conn, if_exists='replace', index=False)

        messagebox.showinfo("Éxito", "Reportes generados y guardados en la base de datos.")

    except Exception as e:
        messagebox.showerror("Error en pipeline", str(e))
        raise e
    finally:
        conn.close()

