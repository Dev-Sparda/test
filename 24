--Estructura de tablas
-- Catálogo de contrapartes
CREATE TABLE CAT_CONTRAPARTES (
    ID_CONTRAPARTE INTEGER PRIMARY KEY AUTOINCREMENT,
    CONTRAPARTE TEXT NOT NULL UNIQUE,
    CLAVE_BANXICO TEXT
);

-- Catálogo de ISIN
CREATE TABLE CAT_ISIN (
    ID_ISIN INTEGER PRIMARY KEY AUTOINCREMENT,
    PAPEL TEXT NOT NULL UNIQUE,
    CLAVE_ISIN TEXT
);

-- Orígenes de datos (CSA, CLEARSTREAM, MORGAN)
CREATE TABLE ORIGEN (
    ID_ORIGEN INTEGER PRIMARY KEY,
    NOMBRE TEXT NOT NULL UNIQUE  -- 'CSA', 'CLEARSTREAM', 'MORGAN'
);

-- Datos crudos normalizados (una sola tabla para los 3 orígenes)
CREATE TABLE MOVIMIENTOS (
    ID_MOVIMIENTO INTEGER PRIMARY KEY AUTOINCREMENT,
    ID_ORIGEN INTEGER NOT NULL,
    FECHA_LIQ DATE NOT NULL,
    BRANCH TEXT,
    CONTRAPARTE TEXT NOT NULL,
    ENVIO_RECEPCION TEXT,  -- 'Envío', 'Recepción', etc.
    EMISION TEXT NOT NULL,
    TITULOS INTEGER,
    ISIN TEXT,  -- solo Morgan lo trae, puede ser NULL en otros
    FOREIGN KEY (ID_ORIGEN) REFERENCES ORIGEN(ID_ORIGEN)
);

-------
UTILS.py
import os
import re

def normalize_emision(emision: str) -> str:
    """Normaliza 'M BONOS 123456' → 'GOBFED 123456' y 'BONOS 123456' → 'GOBFED 123456'"""
    if pd.isna(emision):
        return emision
    emision = str(emision).strip()
    # Caso M BONOS
    emision = re.sub(r'^M\s*BONOS\s+', 'GOBFED ', emision, flags=re.IGNORECASE)
    # Caso solo BONOS (como en Vector PIP)
    emision = re.sub(r'^BONOS\s+', 'GOBFED ', emision, flags=re.IGNORECASE)
    return emision

def classify_emision(emision: str) -> str:
    """Clasifica como 'NACIONAL' si tiene 2 partes, 'EXTRANJERO' si 1"""
    parts = str(emision).split()
    return 'NACIONAL' if len(parts) == 2 else 'EXTRANJERO'

def extract_date_from_pip_filename(filename: str) -> str:
    """Extrae YYYYMMDD de 'PIP20251031M.xls' → '20251031'"""
    import re
    match = re.search(r'PIP(\d{8})M', filename, re.IGNORECASE)
    return match.group(1) if match else None

def full_path_display(path: str) -> str:
    """Muestra ruta completa. Usa 'Raíz' si es directorio raíz (opcional)"""
    if not path:
        return "Raíz"
    return os.path.abspath(path)


-------------------
db.py
import sqlite3
import pandas as pd
from pathlib import Path

DB_PATH = "sistema_colateral.db"

def init_db():
    conn = sqlite3.connect(DB_PATH)
    with conn:
        conn.execute("""
        CREATE TABLE IF NOT EXISTS CAT_CONTRAPARTES (
            ID_CONTRAPARTE INTEGER PRIMARY KEY AUTOINCREMENT,
            CONTRAPARTE TEXT NOT NULL UNIQUE,
            CLAVE_BANXICO TEXT
        )""")

        conn.execute("""
        CREATE TABLE IF NOT EXISTS CAT_ISIN (
            ID_ISIN INTEGER PRIMARY KEY AUTOINCREMENT,
            PAPEL TEXT NOT NULL UNIQUE,
            CLAVE_ISIN TEXT
        )""")

        conn.execute("""
        CREATE TABLE IF NOT EXISTS ORIGEN (
            ID_ORIGEN INTEGER PRIMARY KEY,
            NOMBRE TEXT NOT NULL UNIQUE
        )""")

        conn.execute("INSERT OR IGNORE INTO ORIGEN VALUES (1, 'CSA'), (2, 'CLEARSTREAM'), (3, 'MORGAN')")

        conn.execute("""
        CREATE TABLE IF NOT EXISTS MOVIMIENTOS (
            ID_MOVIMIENTO INTEGER PRIMARY KEY AUTOINCREMENT,
            ID_ORIGEN INTEGER NOT NULL,
            FECHA_LIQ DATE NOT NULL,
            BRANCH TEXT,
            CONTRAPARTE TEXT NOT NULL,
            ENVIO_RECEPCION TEXT,
            EMISION TEXT NOT NULL,
            TITULOS INTEGER,
            ISIN TEXT,
            FOREIGN KEY (ID_ORIGEN) REFERENCES ORIGEN(ID_ORIGEN)
        )""")

        conn.execute("""
        CREATE TABLE IF NOT EXISTS REPORTE_NACIONAL (
            ID INTEGER PRIMARY KEY AUTOINCREMENT,
            CLAVE_BANXICO TEXT,
            CONTRAPARTE TEXT,
            EMISION TEXT,
            TITULOS INTEGER,
            RECIBE_ENTREGA TEXT,
            CLAVE_ISIN TEXT,
            AFORO REAL,
            PRECIO REAL,
            PRECIO_TOTAL REAL,
            VAL_MERCA REAL
        )""")

        conn.execute("""
        CREATE TABLE IF NOT EXISTS REPORTE_EXTRANJERO (
            ID INTEGER PRIMARY KEY AUTOINCREMENT,
            CLAVE_BANXICO TEXT,
            CONTRAPARTE TEXT,
            EMISION TEXT,
            TITULOS INTEGER,
            RECIBE_ENTREGA TEXT,
            CLAVE_ISIN TEXT,
            AFORO REAL,
            PRECIO REAL,
            PRECIO_TOTAL REAL,
            VAL_MERCA REAL
        )""")

    conn.close()

def get_latest_date():
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT MAX(FECHA_LIQ) FROM MOVIMIENTOS")
    result = cursor.fetchone()[0]
    conn.close()
    return result  # Puede ser None (primera carga)


----------------
etl.py

import pandas as pd
import sqlite3
from tkinter import messagebox
from utils import normalize_emision, classify_emision
from db import DB_PATH

ORIGEN_ID = {'CSA': 1, 'CLEARSTREAM': 2, 'MORGAN': 3}

def load_excel_file(filepath: str, origen: str):
    if origen in ['CSA', 'CLEARSTREAM']:
        usecols = "B:K"
        skiprows = 5
    else:  # MORGAN
        usecols = "B:L"
        skiprows = 5

    df = pd.read_excel(filepath, sheet_name='Movimientos', skiprows=skiprows, usecols=usecols)

    # Renombrar columnas comunes
    col_map = {
        'Fecha Liq.': 'FECHA_LIQ',
        'Branch': 'BRANCH',
        'Contraparte': 'CONTRAPARTE',
        'Envío / Recepción': 'ENVIO_RECEPCION',
        'Emisión': 'EMISION',
        '# títulos': 'TITULOS',
        'Haircut': 'HAIRCUT',
        'ISIN': 'ISIN'  # solo en Morgan
    }
    df.rename(columns=col_map, inplace=True)

    # Eliminar columnas que no se usan
    cols_to_drop = ['Precio Mercado', 'Precio c/Haricut', 'Monto $', 'Monto $ USD', 'Monto $ MXN']
    df.drop(columns=[c for c in cols_to_drop if c in df.columns], inplace=True)

    df['EMISION'] = df['EMISION'].apply(normalize_emision)
    df['ID_ORIGEN'] = ORIGEN_ID[origen]

    return df

def validate_and_load_movimientos(df: pd.DataFrame, origen: str):
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # Obtener contrapartes existentes
    existing = set(row[0] for row in cursor.execute("SELECT CONTRAPARTE FROM CAT_CONTRAPARTES").fetchall())

    nuevas_contrapartes = set(df['CONTRAPARTE'].unique()) - existing
    for cp in nuevas_contrapartes:
        if messagebox.askyesno("Contraparte no encontrada",
                               f"No se encontró la contraparte '{cp}' en el catálogo.\n¿Desea darla de alta?"):
            # Aquí podrías pedir CLAVE_BANXICO con un diálogo adicional
            clave_banxico = messagebox.askstring("Clave Banxico", f"Ingrese CLAVE_BANXICO para '{cp}':")
            cursor.execute("INSERT INTO CAT_CONTRAPARTES (CONTRAPARTE, CLAVE_BANXICO) VALUES (?, ?)",
                           (cp, clave_banxico or None))

    # Filtrar fechas nuevas
    latest = pd.read_sql("SELECT MAX(FECHA_LIQ) as max_date FROM MOVIMIENTOS", conn).iloc[0]['max_date']
    if latest:
        df = df[pd.to_datetime(df['FECHA_LIQ']) > pd.to_datetime(latest)]

    # Guardar en MOVIMIENTOS
    df.to_sql('MOVIMIENTOS', conn, if_exists='append', index=False, method='multi')

    conn.commit()
    conn.close()

# etl.py (continuación)

import os
import pandas as pd
import sqlite3
from tkinter import filedialog, messagebox
from utils import normalize_emision, classify_emision, extract_date_from_pip_filename

def run_pipeline(root_window):
    """
    Ejecuta todo el pipeline de generación de reportes.
    root_window: necesario para filedialog desde una app CTk.
    """
    conn = sqlite3.connect(DB_PATH)

    try:
        # 1. Cargar todos los movimientos
        df_mov = pd.read_sql("""
            SELECT 
                m.CONTRAPARTE,
                m.EMISION,
                m.TITULOS,
                m.ISIN,
                m.ENVIO_RECEPCION
            FROM MOVIMIENTOS m
        """, conn)

        if df_mov.empty:
            messagebox.showwarning("Advertencia", "No hay movimientos cargados.")
            return

        # 2. Agrupar por contraparte + emisión
        df_agrupado = df_mov.groupby(['CONTRAPARTE', 'EMISION'], as_index=False).agg({
            'TITULOS': 'sum',
            'ISIN': 'first',
            'ENVIO_RECEPCION': 'first'
        })

        # 3. Clasificar
        df_agrupado['TIPO'] = df_agrupado['EMISION'].apply(classify_emision)
        total_titulos = df_agrupado['TITULOS'].sum()
        messagebox.showinfo("Total", f"Suma total de títulos: {total_titulos:,}")

        # 4. Enriquecer con catálogos
        cat_cp = pd.read_sql("SELECT CONTRAPARTE, CLAVE_BANXICO FROM CAT_CONTRAPARTES", conn)
        cat_isin = pd.read_sql("SELECT PAPEL, CLAVE_ISIN FROM CAT_ISIN", conn)

        df_final = df_agrupado.merge(cat_cp, on='CONTRAPARTE', how='left')
        df_final = df_final.merge(cat_isin, left_on='EMISION', right_on='PAPEL', how='left')

        # 5. Cargar INFORME BONOS para AFORO
        bonos_path = filedialog.askopenfilename(
            parent=root_window,
            title="Selecciona el archivo 'INFORME BONOS'",
            filetypes=[("Excel files", "*.xlsx *.xls")]
        )
        if not bonos_path:
            raise Exception("Archivo de INFORME BONOS no seleccionado.")

        df_bonos = pd.read_excel(bonos_path, sheet_name=0)  # Primera hoja
        df_bonos.rename(columns={'Name': 'EMISION', 'Haircut': 'AFORO'}, inplace=True)
        df_bonos['EMISION'] = df_bonos['EMISION'].apply(normalize_emision)
        df_bonos['CLAVE_AFORO'] = "-" + df_bonos['Saldo inicial (tit.)'].astype(str) + " " + df_bonos['EMISION']

        # Crear clave en nuestro df
        df_final['CLAVE_AFORO'] = df_final['TITULOS'].astype(str) + " " + df_final['EMISION']

        # Merge para AFORO
        df_final = df_final.merge(
            df_bonos[['CLAVE_AFORO', 'AFORO']],
            on='CLAVE_AFORO',
            how='left'
        )

        # 6. Completar AFORO faltante para EXTRANJEROS usando CSA
        missing_extr = df_final[
            (df_final['TIPO'] == 'EXTRANJERO') & (df_final['AFORO'].isna())
        ][['CONTRAPARTE', 'EMISION']]

        if not missing_extr.empty:
            # Cargar CSA (ID_ORIGEN = 1)
            df_csa = pd.read_sql("""
                SELECT CONTRAPARTE, EMISION, HAIRCUT, FECHA_LIQ
                FROM MOVIMIENTOS
                WHERE ID_ORIGEN = 1
            """, conn)

            df_csa['EMISION'] = df_csa['EMISION'].apply(normalize_emision)
            df_csa = df_csa.sort_values('FECHA_LIQ', ascending=False)
            df_csa_dedup = df_csa.drop_duplicates(subset=['CONTRAPARTE', 'EMISION'])

            missing_extr = missing_extr.merge(
                df_csa_dedup[['CONTRAPARTE', 'EMISION', 'HAIRCUT']],
                on=['CONTRAPARTE', 'EMISION'],
                how='left'
            )
            # Convertir 0.900 → 9.0
            missing_extr['HAIRCUT'] = missing_extr['HAIRCUT'] * 100

            # Actualizar AFORO en df_final
            for idx, row in missing_extr.iterrows():
                mask = (df_final['CONTRAPARTE'] == row['CONTRAPARTE']) & \
                       (df_final['EMISION'] == row['EMISION'])
                df_final.loc[mask, 'AFORO'] = row['HAIRCUT']

        # 7. Guardar pendientes NACIONALES sin AFORO
        pendientes_nac = df_final[
            (df_final['TIPO'] == 'NACIONAL') & (df_final['AFORO'].isna())
        ][['CONTRAPARTE', 'EMISION', 'TITULOS']]
        
        if not pendientes_nac.empty:
            output_path = os.path.join(os.path.dirname(bonos_path), "pendientes_por_confirmar_colaterales_aforo.xlsx")
            pendientes_nac.to_excel(output_path, index=False)
            messagebox.showinfo("Pendientes", f"Archivo de pendientes nacional guardado:\n{output_path}")

        # 8. Cargar Vector PIP
        pip_path = filedialog.askopenfilename(
            parent=root_window,
            title="Selecciona el archivo Vector PIP",
            filetypes=[("Excel files", "*.xlsx *.xls")]
        )
        if not pip_path:
            raise Exception("Archivo Vector PIP no seleccionado.")

        pip_date = extract_date_from_pip_filename(os.path.basename(pip_path))
        if not pip_date:
            raise Exception("No se pudo extraer la fecha del archivo PIP. Asegúrate del formato: PIPYYYYMMDDM.xls")

        sheet_name = f"{pip_date}_MD"
        df_pip = pd.read_excel(pip_path, sheet_name=sheet_name)
        df_pip.rename(columns={'Emisora': 'EMISORA', 'Serie': 'SERIE', 'Precio Sucio': 'PRECIO_SUCIO'}, inplace=True)
        df_pip['EMISORA'] = df_pip['EMISORA'].apply(normalize_emision)
        df_pip['CLAVE_PIP'] = df_pip['EMISORA'] + " " + df_pip['SERIE']

        # Obtener tipo de cambio
        tc_row = df_pip[(df_pip['EMISORA'] == 'MXPUSD') & (df_pip['SERIE'] == 'V48')]
        if tc_row.empty:
            raise Exception("No se encontró tipo de cambio (MXPUSD V48) en Vector PIP.")
        tipo_cambio = tc_row.iloc[0]['Precio Limpio']

        # Merge con PIP
        df_final = df_final.merge(
            df_pip[['CLAVE_PIP', 'PRECIO_SUCIO']],
            left_on='EMISION',
            right_on='CLAVE_PIP',
            how='left'
        )

        # 9. Cargar VMDS para precios faltantes
        vmds_path = filedialog.askopenfilename(
            parent=root_window,
            title="Selecciona el archivo VMDS (CSV)",
            filetypes=[("CSV files", "*.csv")]
        )
        if not vmds_path:
            raise Exception("Archivo VMDS no seleccionado.")

        df_vmds = pd.read_csv(vmds_path, header=None, dtype=str)
        if df_vmds.shape[1] < 5:
            raise Exception("El archivo VMDS debe tener al menos 5 columnas.")

        df_vmds.rename(columns={0: 'EMISION_VMDS', 3: 'EMISORA_VMDS', 4: 'SERIE_VMDS'}, inplace=True)
        df_vmds['EMISION_VMDS'] = df_vmds['EMISION_VMDS'].apply(normalize_emision)
        df_vmds['EMISORA_VMDS'] = df_vmds['EMISORA_VMDS'].apply(normalize_emision)
        df_vmds['CLAVE_VMDS'] = df_vmds['EMISORA_VMDS'] + " " + df_vmds['SERIE_VMDS']

        # Filtrar filas sin precio en PIP
        missing_precio = df_final[df_final['PRECIO_SUCIO'].isna()][['EMISION']]
        if not missing_precio.empty:
            # Cruzar con VMDS → PIP
            missing_precio = missing_precio.merge(
                df_vmds[['EMISION_VMDS', 'CLAVE_VMDS']],
                left_on='EMISION',
                right_on='EMISION_VMDS',
                how='inner'
            )
            missing_precio = missing_precio.merge(
                df_pip[['CLAVE_PIP', 'PRECIO_SUCIO']],
                left_on='CLAVE_VMDS',
                right_on='CLAVE_PIP',
                how='left'
            )

            # Dolarizar
            missing_precio['PRECIO_SUCIO'] = pd.to_numeric(missing_precio['PRECIO_SUCIO'], errors='coerce')
            missing_precio['PRECIO_USD'] = missing_precio['PRECIO_SUCIO'] / tipo_cambio

            # Actualizar en df_final
            for idx, row in missing_precio.iterrows():
                mask = df_final['EMISION'] == row['EMISION']
                df_final.loc[mask, 'PRECIO_SUCIO'] = row['PRECIO_USD']

        # 10. Calcular campos finales
        df_final['PRECIO'] = df_final['PRECIO_SUCIO']
        df_final['PRECIO_TOTAL'] = df_final['TITULOS'] * df_final['PRECIO']
        df_final['VAL_MERCA'] = df_final['PRECIO_TOTAL']  # Ajusta si es distinto

        # 11. Separar nacional y extranjero
        nacional = df_final[df_final['TIPO'] == 'NACIONAL'].copy()
        extranjero = df_final[df_final['TIPO'] == 'EXTRANJERO'].copy()

        # Seleccionar columnas requeridas
        cols = ['CLAVE_BANXICO', 'CONTRAPARTE', 'EMISION', 'TITULOS', 'ENVIO_RECEPCION',
                'CLAVE_ISIN', 'AFORO', 'PRECIO', 'PRECIO_TOTAL', 'VAL_MERCA']
        nacional = nacional[cols]
        extranjero = extranjero[cols]

        # Renombrar ENVIO_RECEPCION → RECIBE_ENTREGA
        nacional.rename(columns={'ENVIO_RECEPCION': 'RECIBE_ENTREGA'}, inplace=True)
        extranjero.rename(columns={'ENVIO_RECEPCION': 'RECIBE_ENTREGA'}, inplace=True)

        # 12. Guardar en DB (con TRUNCATE)
        nacional.to_sql('REPORTE_NACIONAL', conn, if_exists='replace', index=False)
        extranjero.to_sql('REPORTE_EXTRANJERO', conn, if_exists='replace', index=False)

        messagebox.showinfo("Éxito", "Reportes generados y guardados en la base de datos.")

    except Exception as e:
        messagebox.showerror("Error en pipeline", str(e))
        raise e
    finally:
        conn.close()

--------------------------------
ui.py

import customtkinter as ctk
from tkinter import filedialog, messagebox
from etl import load_excel_file, validate_and_load_movimientos
from db import init_db

class App(ctk.CTk):
    def __init__(self):
        super().__init__()
        self.title("Sistema de Colateral")
        self.geometry("600x400")

        init_db()

        self.button_csa = ctk.CTkButton(self, text="Cargar CSA", command=lambda: self.load_file("CSA"))
        self.button_csa.pack(pady=10)

        self.button_clear = ctk.CTkButton(self, text="Cargar CLEARSTREAM", command=lambda: self.load_file("CLEARSTREAM"))
        self.button_clear.pack(pady=10)

        self.button_morgan = ctk.CTkButton(self, text="Cargar MORGAN", command=lambda: self.load_file("MORGAN"))
        self.button_morgan.pack(pady=10)

        self.button_run = ctk.CTkButton(self, text="Generar Reportes", command=self.run_pipeline)
        self.button_run.pack(pady=20)

    def load_file(self, origen):
        filepath = filedialog.askopenfilename(filetypes=[("Excel files", "*.xlsx *.xls")])
        if not filepath:
            return
        try:
            df = load_excel_file(filepath, origen)
            validate_and_load_movimientos(df, origen)
            messagebox.showinfo("Éxito", f"Archivo {origen} cargado correctamente.")
        except Exception as e:
            messagebox.showerror("Error", f"Error al cargar {origen}:\n{str(e)}")

    def run_pipeline(self):
        # Aquí iría la lógica completa: agrupar, clasificar, cargar insumos, generar reportes
        messagebox.showinfo("Info", "Pipeline de generación de reportes aún en desarrollo.")
        # → Próximo paso: implementar esto en etl.py

    def run_pipeline(self):
    from etl import run_pipeline
    run_pipeline(self)  # pasa la ventana para filedialog

------------------------------------------
main.py

import customtkinter as ctk
from ui import App

if __name__ == "__main__":
    ctk.set_appearance_mode("System")
    ctk.set_default_color_theme("blue")
    app = App()
    app.mainloop()



------+--
import os
import pandas as pd
from datetime import datetime
import getpass
from pathlib import Path

class ConciliadorFinanciero:
    def __init__(self):
        self.usuario = getpass.getuser().lower()
        self.ruta_base = f"C:/Users/{self.usuario}/Santander Office 365/INFORMACION AUTORIDADES - RR - Area de Trabajo/OPTO/"
        self.mes = None
        self.anio = None
        self.dias_mes = None
        
    def obtener_parametros(self):
        """Solicita mes y año al usuario"""
        self.mes = input("Ingresar el mes de reporte mm. Ej: 01: ")
        self.anio = int(input("Ingresar el año de reporte. Ej: 2021: "))
        self._calcular_dias_mes()
        
    def _calcular_dias_mes(self):
        """Calcula días del mes considerando años bisiestos"""
        meses_31 = ['01', '03', '05', '07', '08', '10', '12']
        meses_30 = ['04', '06', '09', '11']
        
        if self.mes in meses_31:
            self.dias_mes = 31
        elif self.mes in meses_30:
            self.dias_mes = 30
        elif self.mes == '02':
            # Febrero - verificar bisiesto
            if (self.anio % 4 == 0 and self.anio % 100 != 0) or (self.anio % 400 == 0):
                self.dias_mes = 29
            else:
                self.dias_mes = 28
    
    def procesar_caps_floors(self):
        """Equivalente a BOCaps1()"""
        nombre_archivo = f"1-Cap-Floor-al-{self.dias_mes}-de-{self._nombre_mes()}-{self.anio}.xlsx"
        ruta_completa = os.path.join(self.ruta_base, "BO", nombre_archivo)
        
        if os.path.exists(ruta_completa):
            df = pd.read_excel(ruta_completa)
            # Copiar solo valores (sin fórmulas)
            df = df.copy()  # Esto crea una copia explícita
            
            # Aplicar filtros específicos
            df_filtrado = self._aplicar_filtros_bo(df)
            return df_filtrado
        return None
    
    def _aplicar_filtros_bo(self, df):
        """Aplica las reglas de negocio de BO"""
        # 1. Eliminar registros con EVENT_STATUS = "Valid - Front Office"
        if 'EVENT_STATUS' in df.columns:
            df = df[df['EVENT_STATUS'] != 'Valid - Front Office']
        
        # 2. Determinar INSTRUMENTO
        if 'Counterpart' in df.columns:
            df['INSTRUMENTO'] = df['Counterpart'].apply(
                lambda x: 'IMPLICITAS' if 'CLIENTES' in str(x) else 'CAPS'
            )
        
        # 3. Determinar SECCION
        fecha_corte = datetime(2020, 1, 20)
        if 'TradeDate' in df.columns:
            df['SECCION'] = df['TradeDate'].apply(
                lambda x: 'SECCION II' if pd.to_datetime(x) < fecha_corte else 'SECCION IX'
            )
        
        return df
    
    def consolidar_datos(self, *dataframes):
        """Consolida múltiples dataframes en uno solo"""
        # Concatenar todos los dataframes
        df_consolidado = pd.concat(dataframes, ignore_index=True)
        
        # Seleccionar solo columnas necesarias (equivalente al Range delete en VBA)
        columnas_necesarias = [
            'SECCION', 'INSTRUMENTO', 'REGISTRY', 'Trn# (Internal)', 
            'Trn# (External)', 'Desc: Complete Trn. Type', 'Counterpart',
            'Trn.Date', 'SUBYACENTE', 'Date: Period expiry date', 'Clave',
            'Intencion', 'TIPO', 'REGISTRY ORIGINAL', 'STRIKE', 
            'SECCION BX', 'MTM', 'C/V', 'Amortizacion', 'TOTAL MTM', 
            'BROKER FEE', 'TOTAL BROKER FEE'
        ]
        
        # Filtrar columnas existentes
        columnas_existentes = [col for col in columnas_necesarias if col in df_consolidado.columns]
        df_final = df_consolidado[columnas_existentes]
        
        # Ordenar columnas
        df_final = df_final.reindex(columns=columnas_necesarias)
        
        return df_final
    
    def _nombre_mes(self):
        """Devuelve nombre del mes en español"""
        meses = {
            '01': 'Enero', '02': 'Febrero', '03': 'Marzo', '04': 'Abril',
            '05': 'Mayo', '06': 'Junio', '07': 'Julio', '08': 'Agosto',
            '09': 'Septiembre', '10': 'Octubre', '11': 'Noviembre', '12': 'Diciembre'
        }
        return meses.get(self.mes, self.mes)
    
    def generar_reporte(self, df_consolidado):
        """Genera el archivo final de conciliación"""
        nombre_salida = f"CONCILIACION SECCION VII {self._nombre_mes()[:3].lower()} {self.anio}.xlsx"
        ruta_salida = os.path.join(self.ruta_base, "Reportes", nombre_salida)
        
        # Crear directorio si no existe
        os.makedirs(os.path.dirname(ruta_salida), exist_ok=True)
        
        # Guardar con formato
        with pd.ExcelWriter(ruta_salida, engine='openpyxl') as writer:
            df_consolidado.to_excel(writer, sheet_name='Base BO', index=False)
            
            # Aplicar formato (similar al VBA)
            workbook = writer.book
            worksheet = writer.sheets['Base BO']
            
            # Formato de números
            for column in ['MTM', 'TOTAL MTM', 'BROKER FEE', 'TOTAL BROKER FEE']:
                if column in df_consolidado.columns:
                    col_idx = df_consolidado.columns.get_loc(column) + 1
                    for row in range(2, len(df_consolidado) + 2):
                        cell = worksheet.cell(row=row, column=col_idx)
                        cell.number_format = '#,##0.00'
            
            # Autoajustar columnas
            for column_cells in worksheet.columns:
                length = max(len(str(cell.value)) for cell in column_cells)
                worksheet.column_dimensions[column_cells[0].column_letter].width = length + 2
        
        print(f"Reporte generado: {ruta_salida}")
        return ruta_salida
    
    def ejecutar_conciliacion(self):
        """Función principal equivalente a concilpos()"""
        print("=== INICIANDO PROCESO DE CONCILIACIÓN ===\n")
        
        # 1. Obtener parámetros
        self.obtener_parametros()
        
        # 2. Procesar diferentes fuentes de datos
        datos_caps = self.procesar_caps_floors()
        # Aquí irían las llamadas a procesar_swaps(), procesar_oppc(), etc.
        
        # 3. Consolidar todos los datos
        if datos_caps is not None:
            df_consolidado = self.consolidar_datos(datos_caps)
            
            # 4. Generar reporte final
            ruta_reporte = self.generar_reporte(df_consolidado)
            
            print(f"\n=== PROCESO TERMINADO ===")
            print(f"Archivo generado: {ruta_reporte}")
        else:
            print("No se encontraron archivos fuente para procesar.")

# Uso del programa
if __name__ == "__main__":
    conciliador = ConciliadorFinanciero()
    conciliador.ejecutar_conciliacion()


